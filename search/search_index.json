{
    "docs": [
        {
            "location": "/",
            "text": "Dandelion\n\n\nA quite light weight deep learning framework, on top of Theano, offering better balance between flexibility and abstraction.\n\n\nTargeted Users\n\n\nResearchers who need flexibility as well as convenience to experiment all kinds of \nnonstandard\n network structures, and also the stability of Theano.\n\n\nWhy Another DL Framework\n\n\n\n\nThe reason is more about the lack of flexibility for existing DL frameworks, such as Keras, Lasagne, Blocks, etc.\n\n\nBy \n\u201cflexibility\u201d\n, we means whether it is easy to modify or extend the framework. \n\n\nThe famous DL framework Keras is designed to be beginner-friendly oriented, at the cost of being quite hard to modify.\n\n\nCompared to Keras, another less-famous framework Lasagne provides more flexibility. It\u2019s easier to write your own layer by Lasagne for small neural network, however, for complex neural networks it still needs quite manual works because like other existing frameworks, Lasagne operates on abstracted \u2018Layer\u2019 class instead of raw tensor variables.\n\n\n\n\n\n\n\n\nFeaturing\n\n\n\n\nAiming to offer better balance between flexibility and abstraction.\n\n\nEasy to use and extend, support for any neural network structure.  \n\n\nLoose coupling, each part of the framework can be modified independently.\n\n\n\n\n\n\nMore like a handy library of deep learning modules.\n\n\nCommon modules such as CNN, LSTM, GRU, Dense, Dropout, Batch Normalization, and common optimization methods such as SGD, Adam, Adadelta, Rmsprop are ready out-of-the-box.\n\n\n\n\n\n\nPlug & play, operating directly on Theano tensors, no upper abstraction applied.\n\n\nUnlike previous frameworks like Keras, Lasagne, etc., Dandelion operates directly on tensors instead of layer abstractions, making it quite easy to plug in 3rd part defined deep learning modules (layer defined by Keras/Lasagne) or vice versa.\n\n\n\n\n\n\n\n\nProject Layout\n\n\n\n\n\n\n\n\nPython Module\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nmodule\n\n\nall neual network module definitions\n\n\n\n\n\n\nfunctional\n\n\noperations on tensor with no parameter to be learned\n\n\n\n\n\n\ninitialization\n\n\ninitialization methods for neural network modules\n\n\n\n\n\n\nactivation\n\n\ndefinition of all activation functions\n\n\n\n\n\n\nobjective\n\n\ndefinition of all loss objectives\n\n\n\n\n\n\nupdate\n\n\ndefinition of all optimizers\n\n\n\n\n\n\nutil\n\n\nutility functions\n\n\n\n\n\n\nmodel\n\n\nmodel implementations out-of-the-box\n\n\n\n\n\n\n\n\nCredits\n\n\nThe design of Dandelion heavily draws on \nLasagne\n and \nPytorch\n, both my favorate DL libraries.",
            "title": "Home"
        },
        {
            "location": "/#dandelion",
            "text": "A quite light weight deep learning framework, on top of Theano, offering better balance between flexibility and abstraction.",
            "title": "Dandelion"
        },
        {
            "location": "/#targeted-users",
            "text": "Researchers who need flexibility as well as convenience to experiment all kinds of  nonstandard  network structures, and also the stability of Theano.",
            "title": "Targeted Users"
        },
        {
            "location": "/#why-another-dl-framework",
            "text": "The reason is more about the lack of flexibility for existing DL frameworks, such as Keras, Lasagne, Blocks, etc.  By  \u201cflexibility\u201d , we means whether it is easy to modify or extend the framework.   The famous DL framework Keras is designed to be beginner-friendly oriented, at the cost of being quite hard to modify.  Compared to Keras, another less-famous framework Lasagne provides more flexibility. It\u2019s easier to write your own layer by Lasagne for small neural network, however, for complex neural networks it still needs quite manual works because like other existing frameworks, Lasagne operates on abstracted \u2018Layer\u2019 class instead of raw tensor variables.",
            "title": "Why Another DL Framework"
        },
        {
            "location": "/#featuring",
            "text": "Aiming to offer better balance between flexibility and abstraction.  Easy to use and extend, support for any neural network structure.    Loose coupling, each part of the framework can be modified independently.    More like a handy library of deep learning modules.  Common modules such as CNN, LSTM, GRU, Dense, Dropout, Batch Normalization, and common optimization methods such as SGD, Adam, Adadelta, Rmsprop are ready out-of-the-box.    Plug & play, operating directly on Theano tensors, no upper abstraction applied.  Unlike previous frameworks like Keras, Lasagne, etc., Dandelion operates directly on tensors instead of layer abstractions, making it quite easy to plug in 3rd part defined deep learning modules (layer defined by Keras/Lasagne) or vice versa.",
            "title": "Featuring"
        },
        {
            "location": "/#project-layout",
            "text": "Python Module  Explanation      module  all neual network module definitions    functional  operations on tensor with no parameter to be learned    initialization  initialization methods for neural network modules    activation  definition of all activation functions    objective  definition of all loss objectives    update  definition of all optimizers    util  utility functions    model  model implementations out-of-the-box",
            "title": "Project Layout"
        },
        {
            "location": "/#credits",
            "text": "The design of Dandelion heavily draws on  Lasagne  and  Pytorch , both my favorate DL libraries.",
            "title": "Credits"
        },
        {
            "location": "/tutorial I - Sentence Topic Classification/",
            "text": "Tutorial I: Sentence topic classification\n\n\nThe best way to understand how Dandelion works is through practical examples. \n\n\nIn the first part of this tutorial, you\u2019ll be guided through model definition and train/test/predict function compiling with a practical sentence classification task.\n\n\nSentence Classification Task\n\n\n\n\nObjective: classify each sentence into different topic categories.\n\n\nVariant: single-tag classification vs multi-tag classification\n\n\n\n\nThe sentence classification task is to using neural network model to determine the topic of each sentence, i.e., what each sentence is talking about. For example: time, location, cause, action and result.  \n\n\nTo fulfill the task, we\u2019ll build a model basically based on RNN, LSTM specifically. \n\n\nModel Definition - Modules\n\n\nFor the full model definition, check the following code snippet:\n\n\n    import theano\n    import theano.tensor as tensor\n    from dandelion.module import *\n    from dandelion.update import *\n    from dandelion.functional import *\n    from dandelion.util import gpickle\n\n    class model(Module):\n        def __init__(self, batchsize=None, input_length=None, Nclass=6, noise=(0.5, 0.2, 0.7, 0.7, 0.7)):\n            super().__init__()\n            self.batchsize = batchsize\n            self.input_length = input_length\n            self.Nclass = Nclass\n            self.noise = noise\n\n            self.dropout0 = Dropout(name='dropout0')\n            self.dropout1 = Dropout(name='dropout1')\n            self.dropout2 = Dropout(name='dropout2')\n            self.dropout3 = Dropout(name='dropout3')\n            self.dropout4 = Dropout(name='dropout4') \n            W = gpickle.load('word_embedding(6336, 256).gpkl')\n            self.embedding = Embedding(num_embeddings=6336, embedding_dim=256, W=W, name='Embedding')\n            self.lstm0 = LSTM(input_dims=256, hidden_dim=100, name='lstm0')\n            self.lstm1 = LSTM(input_dims=256, hidden_dim=100, name='lstm1')\n            self.lstm2 = LSTM(input_dims=200, hidden_dim=100, name='lstm2')\n            self.lstm3 = LSTM(input_dims=200, hidden_dim=100, name='lstm3')\n            self.lstm4 = LSTM(input_dims=200, hidden_dim=100, name='lstm4')\n            self.lstm5 = LSTM(input_dims=200, hidden_dim=100, name='lstm5')\n            self.dense = Dense(input_dims=200, output_dim=Nclass, name='dense')\n\n\n\n\n\nAll the neural network modules are defined in \ndandelion.module\n in Python. For the sentence classification task, the following four NN modules will be used: \nDropout\n, \nEmbedding\n, \nLSTM\n and \nDense\n.  \n\n\nTo define our model, we\u2019ll need to subclass the \nModule\n class from \ndandelion.module\n. The \nModule\n class is the base class for all our NN modules. There\u2019s no complex abstraction here, all \nModule\n class done is to define some convenient interfaces for model parameter manipulation and no more. The \nModule\n class is quite similar with Pytorch\u2019s \nnn.Module\n class.  \n\n\nNow we define all the network modules as our model\u2019s attributes, such as\n\n\n    self.dropout0 = Dropout(name='dropout0')\n\n\n\n\nYou can drop the name here, it\u2019s optional. However for possible parameter manipulation convenience later, we\u2019d suggest giving a unique name for each network module here. (After version 0.14.0, you don't need to set the module name manually any more, they will be auto-named by the sub-module keys)\n\n\nNote that all these definitions are done in the \nmodel\n\u2019s \n__init__()\n part. Now we defined all the NN modules to be used in our model, but their relations, i.e., the network structure hasn\u2019t been done. This part will be defined in model\u2019s \nforward()\n and \npredict()\n functions later.\n\n\nIf you\u2019re familiar with Lasagne or Keras, you\u2019d notice that for \nLSTM\n module, Dandelion requires both the input dimension via \ninput_dims\n and output dimension via \nhidden_dim\n meanwhile Lasagne or Keras would only require the output dimension, leaving the input dimension determined automatically by the framework. This is the cost you\u2019d pay for greater flexibility by using Dandelion. \n\n\nModel Definition - Structures\n\n\nNow we\u2019ll go through the network structure part. Usually a model needs to be trained first then it can be used in inference, so the network structure would involve these two different processes, i.e., training and inference.\n\n\nWe define the network structure for training in \nModel\n\u2019s \nforward()\n function, as showed below.\n\n\n    def forward(self, x):\n        self.work_mode = 'train'\n        x = self.dropout0.forward(x, p=self.noise[0], rescale=False)\n        x = self.embedding.forward(x)         # (B, T, D)\n\n        x = self.dropout1.forward(x, p=self.noise[1], rescale=True)\n        x = x.dimshuffle((1, 0, 2))           # (B, T, D) -> (T, B, D)\n        x_f = self.lstm0.forward(x, None, None, None)\n        x_b = self.lstm1.forward(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0)\n\n        x = self.dropout2.forward(x, p=self.noise[2], rescale=True)\n        x_f = self.lstm2.forward(x, None, None, None)\n        x_b = self.lstm3.forward(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x = self.dropout3.forward(x, p=self.noise[3], rescale=True)\n        x_f = self.lstm4.forward(x, None, None, None, only_return_final=True)\n        x_b = self.lstm5.forward(x, None, None, None, only_return_final=True, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=1)\n\n        x = self.dropout4.forward(x, p=self.noise[4], rescale=True)\n        y = sigmoid(self.dense.forward(x))\n        return y\n\n\n\n\nWithin the \nforward()\n function, we first set the work mode to \ntrain\n. This is an optional step, which will be explained later. Then the input text sequence is fed through a \nDropout\n and \nEmbedding\n module to convert integer indices into character embedding vectors.  After that are two \nLSTM\n modules with forward and backward scanning directions, resulting in a bidirectional LSTM. Output of this bi-LSTM is then subsampled along the time dimension, and then fed into another bi-LSTM. Note that for the latter bi-LSTM, we only need the last time frame as output. Finally a \nDense\n module followed by a \nsigmoid\n activation gives the sentence classification result.\n\n\nThe network structure can be plotted as\n\n\n\n\nHere the five \nDropout\n modules are plotted with green color, means they only exist during training process.\n\n\n    def predict(self, x):\n        self.work_mode = 'inference'\n        x = self.embedding.predict(x)\n\n        x = x.dimshuffle((1, 0, 2))  # (B, T, D) -> (T, B, D)\n        x_f = self.lstm0.predict(x, None, None, None)\n        x_b = self.lstm1.predict(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0)\n\n        x_f = self.lstm2.predict(x, None, None, None)\n        x_b = self.lstm3.predict(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x_f = self.lstm4.predict(x, None, None, None, only_return_final=True)\n        x_b = self.lstm5.predict(x, None, None, None, only_return_final=True, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=1)\n\n        y = sigmoid(self.dense.predict(x))\n        return y\n\n\n\n\nNow we define the network structure for inference in \nmodel\n\u2019s \npredict()\n function, as showed above.\n\n\nDuring inference process, the \nmodel\n\u2019s network structure is simpler than in training. Note that there\u2019s no \nDropout\n modules here. The rest part of the \npredict()\n function is quite the same with \nforward()\n function, except that now all the modules\u2019 \npredict()\n interface are called instead of the \nforward()\n interface as in \nmodel\n\u2019s \nforward()\n function.\n\n\nUnified Calling Interface\n\n\nFor users familiar with Keras or Lasagne, you might be confused that we define separate functions for both training and inference. In Keras/Lasagne, the common way is to define the model\u2019s structure and use a flag parameter to tell the model to work in training mode or in inference mode.\n\n\nThe reason we do this is because it allows us to use different network structures for different purpose, i.e., the model\u2019s network structure for training can be quite different from the structure for inference.\n\n\nHowever the cost of this flexibility is that we\u2019d have to define the network structure twice even though in most scenarios the model\u2019s network structure is the same for both training and inference.\n\n\nFortunately we\u2019ve considered this and provide a \nunified calling interface\n in Dandelion. For the network structures defined before, they can be re-written by the unified calling interface as follows\n\n\n    def call(self, x, work_mode='train'):\n        self.work_mode = work_mode\n        x = self.dropout0(x, p=self.noise[0], rescale=False)\n        x = self.embedding(x)         # (B, T, D)\n\n        x = self.dropout1(x, p=self.noise[1], rescale=True)\n        x = x.dimshuffle((1, 0, 2))           # (B, T, D) -> (T, B, D)\n        x_f = self.lstm0(x, None, None, None)\n        x_b = self.lstm1(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0)\n\n        x = self.dropout2(x, p=self.noise[2], rescale=True)\n        x_f = self.lstm2(x, None, None, None)\n        x_b = self.lstm3(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x = self.dropout3(x, p=self.noise[3], rescale=True)\n        x_f = self.lstm4(x, None, None, None, only_return_final=True)\n        x_b = self.lstm5(x, None, None, None, only_return_final=True, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=1)\n\n        x = self.dropout4(x, p=self.noise[4], rescale=True)\n        y = sigmoid(self.dense(x))\n        return y\n\n\n\n\nAs we can see from the code above, now we do not call the sub-module\u2019s \nforward()\n or \npredict()\n interface anymore. By setting the \nwork_mode\n parameter, Dandelion will automatically call the sub-module\u2019s \nforward()\n or \npredict()\n interface accordingly. Now we only need define the network structure for once, and use it for both training and inference.\n\n\nModel Compiling\n\n\nTheano requires compiling the computation graph before using it. The model compiling is actually more relevant to Theano than to Dandelion. \n\n\n    print('  compiling train func')\n    X            = tensor.imatrix('X')  \n    Y            = tensor.fmatrix('Y')       \n    output_score = model.forward(X)          \n    B            = Y.shape[0]\n    B            = tensor.cast(B, 'float32')\n    loss         = tensor.sqrt(tensor.sum((output_score - Y)**2)) / B * 100                                        \n\n    Y_out_positive = tensor.zeros_like(output_score)\n    Y_out_positive = tensor.switch(output_score>0.5, Y, Y_out_positive)\n    acc_positive   = tensor.sum(Y_out_positive) / tensor.sum(Y)\n\n    Y_out_negative = tensor.zeros_like(output_score)\n    Y_out_negative = tensor.switch(output_score<=0.5, 1.0 - Y, Y_out_negative)\n    acc_negative   = tensor.sum(Y_out_negative) / tensor.sum(1.0- Y)\n\n    params       = model.collect_params()\n    updates      = adadelta(loss, params)\n    updates.update(model.collect_self_updates())\n    train_fn = theano.function([X, Y], [loss, acc_positive, acc_negative], updates=updates, no_default_updates=False)\n\n\n\n\n\n\n\n\nModel Calling\n\nHere we call the model defined before by \nmodel.forward()\n. Of course you can also call the model by the unified calling interface as \nmodel.call(\u2026, work_mode=\u2018train\u2019)\n\n\n\n\n\n\nParameters Collecting\n\nParameters to be trained by optimizer can be collected from the model by calling \nmodel.collect_params()\n, simply like that.\n\n\n\n\n\n\nUpdates Collecting\n\nIn Dandelion, there\u2019re two kinds of parameters: parameters to be updated by optimizer and parameters to be updated by other methods. The updates expression of the latter part of parameters can be collected by calling \nmodel.collect_self_updates()\n. Returned is a \ndict\n describing updates for each parameter accordingly.\nAfter these 3 steps, now we can compile the training function by Theano simply by\n\n\n\n\n\n\n    train_fn = theano.function([X, Y], [loss, acc_positive, acc_negative], updates=updates, no_default_updates=False)",
            "title": "I - Sentence Topic Classification"
        },
        {
            "location": "/tutorial I - Sentence Topic Classification/#tutorial-i-sentence-topic-classification",
            "text": "The best way to understand how Dandelion works is through practical examples.   In the first part of this tutorial, you\u2019ll be guided through model definition and train/test/predict function compiling with a practical sentence classification task.",
            "title": "Tutorial I: Sentence topic classification"
        },
        {
            "location": "/tutorial I - Sentence Topic Classification/#sentence-classification-task",
            "text": "Objective: classify each sentence into different topic categories.  Variant: single-tag classification vs multi-tag classification   The sentence classification task is to using neural network model to determine the topic of each sentence, i.e., what each sentence is talking about. For example: time, location, cause, action and result.    To fulfill the task, we\u2019ll build a model basically based on RNN, LSTM specifically.",
            "title": "Sentence Classification Task"
        },
        {
            "location": "/tutorial I - Sentence Topic Classification/#model-definition-modules",
            "text": "For the full model definition, check the following code snippet:      import theano\n    import theano.tensor as tensor\n    from dandelion.module import *\n    from dandelion.update import *\n    from dandelion.functional import *\n    from dandelion.util import gpickle\n\n    class model(Module):\n        def __init__(self, batchsize=None, input_length=None, Nclass=6, noise=(0.5, 0.2, 0.7, 0.7, 0.7)):\n            super().__init__()\n            self.batchsize = batchsize\n            self.input_length = input_length\n            self.Nclass = Nclass\n            self.noise = noise\n\n            self.dropout0 = Dropout(name='dropout0')\n            self.dropout1 = Dropout(name='dropout1')\n            self.dropout2 = Dropout(name='dropout2')\n            self.dropout3 = Dropout(name='dropout3')\n            self.dropout4 = Dropout(name='dropout4') \n            W = gpickle.load('word_embedding(6336, 256).gpkl')\n            self.embedding = Embedding(num_embeddings=6336, embedding_dim=256, W=W, name='Embedding')\n            self.lstm0 = LSTM(input_dims=256, hidden_dim=100, name='lstm0')\n            self.lstm1 = LSTM(input_dims=256, hidden_dim=100, name='lstm1')\n            self.lstm2 = LSTM(input_dims=200, hidden_dim=100, name='lstm2')\n            self.lstm3 = LSTM(input_dims=200, hidden_dim=100, name='lstm3')\n            self.lstm4 = LSTM(input_dims=200, hidden_dim=100, name='lstm4')\n            self.lstm5 = LSTM(input_dims=200, hidden_dim=100, name='lstm5')\n            self.dense = Dense(input_dims=200, output_dim=Nclass, name='dense')  All the neural network modules are defined in  dandelion.module  in Python. For the sentence classification task, the following four NN modules will be used:  Dropout ,  Embedding ,  LSTM  and  Dense .    To define our model, we\u2019ll need to subclass the  Module  class from  dandelion.module . The  Module  class is the base class for all our NN modules. There\u2019s no complex abstraction here, all  Module  class done is to define some convenient interfaces for model parameter manipulation and no more. The  Module  class is quite similar with Pytorch\u2019s  nn.Module  class.    Now we define all the network modules as our model\u2019s attributes, such as      self.dropout0 = Dropout(name='dropout0')  You can drop the name here, it\u2019s optional. However for possible parameter manipulation convenience later, we\u2019d suggest giving a unique name for each network module here. (After version 0.14.0, you don't need to set the module name manually any more, they will be auto-named by the sub-module keys)  Note that all these definitions are done in the  model \u2019s  __init__()  part. Now we defined all the NN modules to be used in our model, but their relations, i.e., the network structure hasn\u2019t been done. This part will be defined in model\u2019s  forward()  and  predict()  functions later.  If you\u2019re familiar with Lasagne or Keras, you\u2019d notice that for  LSTM  module, Dandelion requires both the input dimension via  input_dims  and output dimension via  hidden_dim  meanwhile Lasagne or Keras would only require the output dimension, leaving the input dimension determined automatically by the framework. This is the cost you\u2019d pay for greater flexibility by using Dandelion.",
            "title": "Model Definition - Modules"
        },
        {
            "location": "/tutorial I - Sentence Topic Classification/#model-definition-structures",
            "text": "Now we\u2019ll go through the network structure part. Usually a model needs to be trained first then it can be used in inference, so the network structure would involve these two different processes, i.e., training and inference.  We define the network structure for training in  Model \u2019s  forward()  function, as showed below.      def forward(self, x):\n        self.work_mode = 'train'\n        x = self.dropout0.forward(x, p=self.noise[0], rescale=False)\n        x = self.embedding.forward(x)         # (B, T, D)\n\n        x = self.dropout1.forward(x, p=self.noise[1], rescale=True)\n        x = x.dimshuffle((1, 0, 2))           # (B, T, D) -> (T, B, D)\n        x_f = self.lstm0.forward(x, None, None, None)\n        x_b = self.lstm1.forward(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0)\n\n        x = self.dropout2.forward(x, p=self.noise[2], rescale=True)\n        x_f = self.lstm2.forward(x, None, None, None)\n        x_b = self.lstm3.forward(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x = self.dropout3.forward(x, p=self.noise[3], rescale=True)\n        x_f = self.lstm4.forward(x, None, None, None, only_return_final=True)\n        x_b = self.lstm5.forward(x, None, None, None, only_return_final=True, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=1)\n\n        x = self.dropout4.forward(x, p=self.noise[4], rescale=True)\n        y = sigmoid(self.dense.forward(x))\n        return y  Within the  forward()  function, we first set the work mode to  train . This is an optional step, which will be explained later. Then the input text sequence is fed through a  Dropout  and  Embedding  module to convert integer indices into character embedding vectors.  After that are two  LSTM  modules with forward and backward scanning directions, resulting in a bidirectional LSTM. Output of this bi-LSTM is then subsampled along the time dimension, and then fed into another bi-LSTM. Note that for the latter bi-LSTM, we only need the last time frame as output. Finally a  Dense  module followed by a  sigmoid  activation gives the sentence classification result.  The network structure can be plotted as   Here the five  Dropout  modules are plotted with green color, means they only exist during training process.      def predict(self, x):\n        self.work_mode = 'inference'\n        x = self.embedding.predict(x)\n\n        x = x.dimshuffle((1, 0, 2))  # (B, T, D) -> (T, B, D)\n        x_f = self.lstm0.predict(x, None, None, None)\n        x_b = self.lstm1.predict(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0)\n\n        x_f = self.lstm2.predict(x, None, None, None)\n        x_b = self.lstm3.predict(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x_f = self.lstm4.predict(x, None, None, None, only_return_final=True)\n        x_b = self.lstm5.predict(x, None, None, None, only_return_final=True, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=1)\n\n        y = sigmoid(self.dense.predict(x))\n        return y  Now we define the network structure for inference in  model \u2019s  predict()  function, as showed above.  During inference process, the  model \u2019s network structure is simpler than in training. Note that there\u2019s no  Dropout  modules here. The rest part of the  predict()  function is quite the same with  forward()  function, except that now all the modules\u2019  predict()  interface are called instead of the  forward()  interface as in  model \u2019s  forward()  function.",
            "title": "Model Definition - Structures"
        },
        {
            "location": "/tutorial I - Sentence Topic Classification/#unified-calling-interface",
            "text": "For users familiar with Keras or Lasagne, you might be confused that we define separate functions for both training and inference. In Keras/Lasagne, the common way is to define the model\u2019s structure and use a flag parameter to tell the model to work in training mode or in inference mode.  The reason we do this is because it allows us to use different network structures for different purpose, i.e., the model\u2019s network structure for training can be quite different from the structure for inference.  However the cost of this flexibility is that we\u2019d have to define the network structure twice even though in most scenarios the model\u2019s network structure is the same for both training and inference.  Fortunately we\u2019ve considered this and provide a  unified calling interface  in Dandelion. For the network structures defined before, they can be re-written by the unified calling interface as follows      def call(self, x, work_mode='train'):\n        self.work_mode = work_mode\n        x = self.dropout0(x, p=self.noise[0], rescale=False)\n        x = self.embedding(x)         # (B, T, D)\n\n        x = self.dropout1(x, p=self.noise[1], rescale=True)\n        x = x.dimshuffle((1, 0, 2))           # (B, T, D) -> (T, B, D)\n        x_f = self.lstm0(x, None, None, None)\n        x_b = self.lstm1(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x = pool_1d(x, ws=2, ignore_border=True, mode='average_exc_pad', axis=0)\n\n        x = self.dropout2(x, p=self.noise[2], rescale=True)\n        x_f = self.lstm2(x, None, None, None)\n        x_b = self.lstm3(x, None, None, None, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=2)\n\n        x = self.dropout3(x, p=self.noise[3], rescale=True)\n        x_f = self.lstm4(x, None, None, None, only_return_final=True)\n        x_b = self.lstm5(x, None, None, None, only_return_final=True, backward=True)\n        x = tensor.concatenate([x_f, x_b], axis=1)\n\n        x = self.dropout4(x, p=self.noise[4], rescale=True)\n        y = sigmoid(self.dense(x))\n        return y  As we can see from the code above, now we do not call the sub-module\u2019s  forward()  or  predict()  interface anymore. By setting the  work_mode  parameter, Dandelion will automatically call the sub-module\u2019s  forward()  or  predict()  interface accordingly. Now we only need define the network structure for once, and use it for both training and inference.",
            "title": "Unified Calling Interface"
        },
        {
            "location": "/tutorial I - Sentence Topic Classification/#model-compiling",
            "text": "Theano requires compiling the computation graph before using it. The model compiling is actually more relevant to Theano than to Dandelion.       print('  compiling train func')\n    X            = tensor.imatrix('X')  \n    Y            = tensor.fmatrix('Y')       \n    output_score = model.forward(X)          \n    B            = Y.shape[0]\n    B            = tensor.cast(B, 'float32')\n    loss         = tensor.sqrt(tensor.sum((output_score - Y)**2)) / B * 100                                        \n\n    Y_out_positive = tensor.zeros_like(output_score)\n    Y_out_positive = tensor.switch(output_score>0.5, Y, Y_out_positive)\n    acc_positive   = tensor.sum(Y_out_positive) / tensor.sum(Y)\n\n    Y_out_negative = tensor.zeros_like(output_score)\n    Y_out_negative = tensor.switch(output_score<=0.5, 1.0 - Y, Y_out_negative)\n    acc_negative   = tensor.sum(Y_out_negative) / tensor.sum(1.0- Y)\n\n    params       = model.collect_params()\n    updates      = adadelta(loss, params)\n    updates.update(model.collect_self_updates())\n    train_fn = theano.function([X, Y], [loss, acc_positive, acc_negative], updates=updates, no_default_updates=False)    Model Calling \nHere we call the model defined before by  model.forward() . Of course you can also call the model by the unified calling interface as  model.call(\u2026, work_mode=\u2018train\u2019)    Parameters Collecting \nParameters to be trained by optimizer can be collected from the model by calling  model.collect_params() , simply like that.    Updates Collecting \nIn Dandelion, there\u2019re two kinds of parameters: parameters to be updated by optimizer and parameters to be updated by other methods. The updates expression of the latter part of parameters can be collected by calling  model.collect_self_updates() . Returned is a  dict  describing updates for each parameter accordingly.\nAfter these 3 steps, now we can compile the training function by Theano simply by        train_fn = theano.function([X, Y], [loss, acc_positive, acc_negative], updates=updates, no_default_updates=False)",
            "title": "Model Compiling"
        },
        {
            "location": "/tutorial II - Write Your Own Module/",
            "text": "Tutorial II: Write Your Own Module\n\n\nIn this tutorial, you\u2019ll learn how to write your own neural network module with the help of Dandelion. Here we\u2019ll design a module which gives the class centers for classification output. It\u2019s a simple case for Dandelion yet not so intuitive for Lasagne or Keras users.\n\n\nIn image classification tasks, such as face recognition, document image classification, Imagenet contests, etc., we usually consider only the \u201cpositive\u201d samples, i.e., we assume that given any input sample, it would be associated with at least one out of all the known class labels. However, in actual applications, we often also want the trained neural network model to be able to tell whether an input sample is an \u201coutsider\u201d or not.\n\n\nTo accomplish this task, we can add an extra \u201cnegative\u201d class to the final layer of the network, and then train this augmented network by feeding it with all kinds of \u201cnegative\u201d samples you can collect. It\u2019s pure data-driven, so the bottleneck is how many \u201cnegative\u201d samples can be collected. \n\n\nAnother way is algorithm-driven: we design a new network module to explore the intrinsic properties of the data, and use these \u201cproperties\u201d to reject or accept an sample as \u201cpositive\u201d. By this way we do not need to collect negative samples, and the model is more general and the most important: explainable.\n\n\nThe data intrinsic property to explore here is the class center for each positive class. The intuition is that if we can get the center of each class, then we can use the sample-center distance to reject or accept an sample as \u201cpositive\u201d. \n\n\nNow assume that the last layer of the neural network is a \nDense\n module followed by a \nsoftmax\n activation which produces \nN\n class decisions. We\u2019ll refer the input of this \nDense\n module as feature of the input sample (extracted by the former part of the whole neural network). For plain network trained with only positive samples, the feature distribution can be typically visualized as\n\n\n\n\n\n\nA Discriminative Deep Feature Learning Approach for Face Recognitions\n. Yandong Wen, Kaipeng Zhang, Zhifeng Li and Yu Qiao. European Conference on Computer Vision (ECCV) 2016\n\n\n\n\nCenter Loss\n\n\nApparently the feature extracted by the plain model is not well centered, in other words, the feature distribution is not well-formed.\n\n\nIdeally, to reject or accept one sample as a certain class, we can set a probability threshold so that any sample whose feature satisfies\n\ud835\udc5d(\ud835\udc53_\ud835\udc57\u2502\ud835\udc36_\ud835\udc56)<\ud835\udc47_\ud835\udc56 will be rejected as an \u201coutsider\u201d for this class with certainty 1\u2212\ud835\udc47_\ud835\udc56\n\n\nBut before we can do this, the distribution \ud835\udc5d(\ud835\udc53\u2502\ud835\udc36_\ud835\udc56) must be known. To get this conditional distribution, we can either traverse all the train samples and use any probability estimation / modelling method to approximate the true distribution, or we can resort to the DL method by directly requiring the neural network to produce features satisfying predefined distributions.\n\n\nThe reason we can do this is because a neural network can be trained to emulate any nonlinear functions, and we can always transform a compact distribution into Gaussian by a certain function. \n\n\nTo restrain the neural network to extract Gaussian distributed features, we assume each class has a mean feature vector (i.e., center) \ud835\udc53_\ud835\udf07\ud835\udc56 and require the model to minimize the distance between extracted feature and its corresponding center vector, i.e., \n\n\nmin\u2061\u2016\ud835\udc53_\ud835\udc57\u2212\ud835\udc53_\ud835\udf07\ud835\udc56 \u2016^2  \ud835\udc56\ud835\udc53 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 \ud835\udc57 \ud835\udc4f\ud835\udc52\ud835\udc59\ud835\udc5c\ud835\udc5b\ud835\udc54\ud835\udc60 \ud835\udc61\ud835\udc5c \ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc56\n\n\nWe refer this objective as \u201ccenter loss\u201d, the details can be found in Ref. [A Discriminative Deep Feature Learning Approach for Face Recognition. Yandong Wen, Kaipeng Zhang, Zhifeng Li and Yu Qiao. European Conference on Computer Vision (ECCV) 2016]. The model is trained now with both the categorical cross entropy loss and the center loss as \n\n\nmin\u2061 \ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc54\ud835\udc5c\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc36\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc66+\ud835\udf06\u2217\ud835\udc36\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60  \n\n\n\n\nCenter\n Module\n\n\nNow we\u2019ll go through the code part to illustrate how the center loss can be actually computed. To compute the center loss, we need first to get the center estimation of each class. This is done through a new module referred as \nCenter\n. Check the code snippet following.\n\n\nclass Center(Module):\n    \"\"\"\n    Compute the class centers during training\n    Ref. to \"Discriminative feature learning approach for deep face recognition (2016)\"\n    \"\"\"\n    def __init__(self, feature_dim, center_num, alpha=0.9, center=init.GlorotUniform(), name=None):\n        \"\"\"\n        :param alpha: moving averaging coefficient\n        :param center: initial value of center\n        \"\"\"\n        super().__init__(name=name)\n        self.center = self.register_self_updating_variable(center, shape=[center_num, feature_dim], name=\"center\")\n        self.alpha = alpha\n\n    def forward(self, features, labels):\n        \"\"\"\n        :param features: (B, D)\n        :param labels: (B,)\n        :return: categorical centers\n        \"\"\"\n        center_batch = self.center[labels, :]\n        diff = (self.alpha - 1.0) * (center_batch - features)\n        center_updated = tensor.inc_subtensor(self.center[labels, :], diff)\n        self.center.default_update = center_updated\n        return self.center\n\n    def predict(self):\n        return self.center\n\n\n\n\n\nFirst, all our NN modules should subclass the root \nModule\n class, then we can use class methods and attributes to manipulate network parameters conveniently.\n\n\nSecond, define the module initialization in \n.__init__()\n part. Here we do two things: we register a \ncenter\n tensor as network parameter and initialize it with a Glorot uniform random numpy array. The \ncenter\n tensor is of shape \n(center_num, feature_dim)\n, in which \ncenter_num\n should be equal to class number, and \nfeature_dim\n is the dimension of extracted features by the network.\n\n\nIn Dandelion, the network parameters are divided into two categories:  \n\n\n\n\n1) parameter to be updated by optimizer,  \n\n\n2) parameter to updated by user defined expression.\n\n\n\n\nThe former parameters should be registered with class method \n.register_param()\n, and the latter parameters should be registered with class method \n. register_self_updating_variable()\n.\n\n\nNow we registered \ncenter\n tensor as self updating variable, its updating expression is given in \n.forward()\n function as \nself.center.update = center_updated\n. In Dandelion we use a specially named attribute \n. update\n to tell the framework that this parameter has an updating expression defined and the updating expression will be collected during Theano function compiling phase.\n\n\nThe \n.forward()\n function will be used for training, and \n.predict()\n function will be used for inference. \n\n\nBasically, during training, the \n.forward()\n function computes moving averaging estimation of class centers; and during inference, we just use the stored center values as final estimated class centers. This is pretty much alike how \nBatchNorm\n\u2019s mean and std are estimated and used.\n\n\nSummary\n\n\nTo summary, to write your own module, you only need to do the following three steps:\n\n\n\n\n1) subclass \nModule\n class\n\n\n2) register your module\u2019s parameters by \n.register_param()\n or \n. register_self_updating_variable()\n and initialize them\n\n\n3) define the \n.forward()\n function for training and \n.predict()\n function for inference\n\n\n\n\nand that\u2019s it!",
            "title": "II - Write Your Own Module"
        },
        {
            "location": "/tutorial II - Write Your Own Module/#tutorial-ii-write-your-own-module",
            "text": "In this tutorial, you\u2019ll learn how to write your own neural network module with the help of Dandelion. Here we\u2019ll design a module which gives the class centers for classification output. It\u2019s a simple case for Dandelion yet not so intuitive for Lasagne or Keras users.  In image classification tasks, such as face recognition, document image classification, Imagenet contests, etc., we usually consider only the \u201cpositive\u201d samples, i.e., we assume that given any input sample, it would be associated with at least one out of all the known class labels. However, in actual applications, we often also want the trained neural network model to be able to tell whether an input sample is an \u201coutsider\u201d or not.  To accomplish this task, we can add an extra \u201cnegative\u201d class to the final layer of the network, and then train this augmented network by feeding it with all kinds of \u201cnegative\u201d samples you can collect. It\u2019s pure data-driven, so the bottleneck is how many \u201cnegative\u201d samples can be collected.   Another way is algorithm-driven: we design a new network module to explore the intrinsic properties of the data, and use these \u201cproperties\u201d to reject or accept an sample as \u201cpositive\u201d. By this way we do not need to collect negative samples, and the model is more general and the most important: explainable.  The data intrinsic property to explore here is the class center for each positive class. The intuition is that if we can get the center of each class, then we can use the sample-center distance to reject or accept an sample as \u201cpositive\u201d.   Now assume that the last layer of the neural network is a  Dense  module followed by a  softmax  activation which produces  N  class decisions. We\u2019ll refer the input of this  Dense  module as feature of the input sample (extracted by the former part of the whole neural network). For plain network trained with only positive samples, the feature distribution can be typically visualized as    A Discriminative Deep Feature Learning Approach for Face Recognitions . Yandong Wen, Kaipeng Zhang, Zhifeng Li and Yu Qiao. European Conference on Computer Vision (ECCV) 2016",
            "title": "Tutorial II: Write Your Own Module"
        },
        {
            "location": "/tutorial II - Write Your Own Module/#center-loss",
            "text": "Apparently the feature extracted by the plain model is not well centered, in other words, the feature distribution is not well-formed.  Ideally, to reject or accept one sample as a certain class, we can set a probability threshold so that any sample whose feature satisfies\n\ud835\udc5d(\ud835\udc53_\ud835\udc57\u2502\ud835\udc36_\ud835\udc56)<\ud835\udc47_\ud835\udc56 will be rejected as an \u201coutsider\u201d for this class with certainty 1\u2212\ud835\udc47_\ud835\udc56  But before we can do this, the distribution \ud835\udc5d(\ud835\udc53\u2502\ud835\udc36_\ud835\udc56) must be known. To get this conditional distribution, we can either traverse all the train samples and use any probability estimation / modelling method to approximate the true distribution, or we can resort to the DL method by directly requiring the neural network to produce features satisfying predefined distributions.  The reason we can do this is because a neural network can be trained to emulate any nonlinear functions, and we can always transform a compact distribution into Gaussian by a certain function.   To restrain the neural network to extract Gaussian distributed features, we assume each class has a mean feature vector (i.e., center) \ud835\udc53_\ud835\udf07\ud835\udc56 and require the model to minimize the distance between extracted feature and its corresponding center vector, i.e.,   min\u2061\u2016\ud835\udc53_\ud835\udc57\u2212\ud835\udc53_\ud835\udf07\ud835\udc56 \u2016^2  \ud835\udc56\ud835\udc53 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 \ud835\udc57 \ud835\udc4f\ud835\udc52\ud835\udc59\ud835\udc5c\ud835\udc5b\ud835\udc54\ud835\udc60 \ud835\udc61\ud835\udc5c \ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc56  We refer this objective as \u201ccenter loss\u201d, the details can be found in Ref. [A Discriminative Deep Feature Learning Approach for Face Recognition. Yandong Wen, Kaipeng Zhang, Zhifeng Li and Yu Qiao. European Conference on Computer Vision (ECCV) 2016]. The model is trained now with both the categorical cross entropy loss and the center loss as   min\u2061 \ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc54\ud835\udc5c\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc36\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc66+\ud835\udf06\u2217\ud835\udc36\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60",
            "title": "Center Loss"
        },
        {
            "location": "/tutorial II - Write Your Own Module/#center-module",
            "text": "Now we\u2019ll go through the code part to illustrate how the center loss can be actually computed. To compute the center loss, we need first to get the center estimation of each class. This is done through a new module referred as  Center . Check the code snippet following.  class Center(Module):\n    \"\"\"\n    Compute the class centers during training\n    Ref. to \"Discriminative feature learning approach for deep face recognition (2016)\"\n    \"\"\"\n    def __init__(self, feature_dim, center_num, alpha=0.9, center=init.GlorotUniform(), name=None):\n        \"\"\"\n        :param alpha: moving averaging coefficient\n        :param center: initial value of center\n        \"\"\"\n        super().__init__(name=name)\n        self.center = self.register_self_updating_variable(center, shape=[center_num, feature_dim], name=\"center\")\n        self.alpha = alpha\n\n    def forward(self, features, labels):\n        \"\"\"\n        :param features: (B, D)\n        :param labels: (B,)\n        :return: categorical centers\n        \"\"\"\n        center_batch = self.center[labels, :]\n        diff = (self.alpha - 1.0) * (center_batch - features)\n        center_updated = tensor.inc_subtensor(self.center[labels, :], diff)\n        self.center.default_update = center_updated\n        return self.center\n\n    def predict(self):\n        return self.center  First, all our NN modules should subclass the root  Module  class, then we can use class methods and attributes to manipulate network parameters conveniently.  Second, define the module initialization in  .__init__()  part. Here we do two things: we register a  center  tensor as network parameter and initialize it with a Glorot uniform random numpy array. The  center  tensor is of shape  (center_num, feature_dim) , in which  center_num  should be equal to class number, and  feature_dim  is the dimension of extracted features by the network.  In Dandelion, the network parameters are divided into two categories:     1) parameter to be updated by optimizer,    2) parameter to updated by user defined expression.   The former parameters should be registered with class method  .register_param() , and the latter parameters should be registered with class method  . register_self_updating_variable() .  Now we registered  center  tensor as self updating variable, its updating expression is given in  .forward()  function as  self.center.update = center_updated . In Dandelion we use a specially named attribute  . update  to tell the framework that this parameter has an updating expression defined and the updating expression will be collected during Theano function compiling phase.  The  .forward()  function will be used for training, and  .predict()  function will be used for inference.   Basically, during training, the  .forward()  function computes moving averaging estimation of class centers; and during inference, we just use the stored center values as final estimated class centers. This is pretty much alike how  BatchNorm \u2019s mean and std are estimated and used.",
            "title": "Center Module"
        },
        {
            "location": "/tutorial II - Write Your Own Module/#summary",
            "text": "To summary, to write your own module, you only need to do the following three steps:   1) subclass  Module  class  2) register your module\u2019s parameters by  .register_param()  or  . register_self_updating_variable()  and initialize them  3) define the  .forward()  function for training and  .predict()  function for inference   and that\u2019s it!",
            "title": "Summary"
        },
        {
            "location": "/howtos/",
            "text": "Tutorial III: Howtos\n\n\n1) How to freeze a module during training like in Keras/Lasagne?\n\n\nTo \nfreeze\n a module during training, use the \ninclude\n and \nexclude\n arguments of module's \n.collect_params()\n and \n.collect_self_updates()\n functions.\n\n\nExample\n\n\nclass FOO(Module):\n    def __init__(self):\n        self.cnn0 = Conv2D(...)\n        self.cnn1 = Conv2D(...)\n        self.cnn2 = Conv2D(...)\n        ....\n\n# Now we will freeze cnn0 and cnn1 submodules during training\nmodel    = Foo()\nloss     = ...\nparams   = model.collect_params(exclude=['cnn0', 'cnn1'])\nupdates  = optimizer(loss, params)\nupdates.update(model.colect_self_updates(exclude=['cnn0', 'cnn1']))\ntrain_fn = theano.function([...], [...], updates=updates, no_default_updates=False)\n\n\n\n\n2) How to add random noise to a tensor?\n\n\nJust use Theano's \nMRG_RandomStreams\n module.\n\n\nExample\n\n\nfrom theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\nsrng = RandomStreams(np.random.randint(1, 2147462579))\n....\ny = x + srng.normal(x.shape, avg=0.0, std=0.1)   # add Gaussian noise to x\n\n\n\n\nWhat you'd keep in mind is that if you used Theano's \nMRG_RandomStreams\n module, remember to set \nno_default_updates=False\n when compiling functions.",
            "title": "III - Howtos"
        },
        {
            "location": "/howtos/#tutorial-iii-howtos",
            "text": "",
            "title": "Tutorial III: Howtos"
        },
        {
            "location": "/howtos/#1-how-to-freeze-a-module-during-training-like-in-keraslasagne",
            "text": "To  freeze  a module during training, use the  include  and  exclude  arguments of module's  .collect_params()  and  .collect_self_updates()  functions.",
            "title": "1) How to freeze a module during training like in Keras/Lasagne?"
        },
        {
            "location": "/howtos/#example",
            "text": "class FOO(Module):\n    def __init__(self):\n        self.cnn0 = Conv2D(...)\n        self.cnn1 = Conv2D(...)\n        self.cnn2 = Conv2D(...)\n        ....\n\n# Now we will freeze cnn0 and cnn1 submodules during training\nmodel    = Foo()\nloss     = ...\nparams   = model.collect_params(exclude=['cnn0', 'cnn1'])\nupdates  = optimizer(loss, params)\nupdates.update(model.colect_self_updates(exclude=['cnn0', 'cnn1']))\ntrain_fn = theano.function([...], [...], updates=updates, no_default_updates=False)",
            "title": "Example"
        },
        {
            "location": "/howtos/#2-how-to-add-random-noise-to-a-tensor",
            "text": "Just use Theano's  MRG_RandomStreams  module.",
            "title": "2) How to add random noise to a tensor?"
        },
        {
            "location": "/howtos/#example_1",
            "text": "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\nsrng = RandomStreams(np.random.randint(1, 2147462579))\n....\ny = x + srng.normal(x.shape, avg=0.0, std=0.1)   # add Gaussian noise to x  What you'd keep in mind is that if you used Theano's  MRG_RandomStreams  module, remember to set  no_default_updates=False  when compiling functions.",
            "title": "Example"
        },
        {
            "location": "/dandelion_module/",
            "text": "Module\n\n\nRoot class of all network modules, you'd always subclass this for a new module\n\n\nclass Module(name=None, work_mode='inference')\n\n\n\n\n\n\nname\n: module name, optional. If you don't specify the module name, it will be auto-named if this module is a sub-module of another module.\n\n\nwork_mode\n: working mode, optional. Only used for the unified calling interface, check \"Tutorial I\" for detailed explanation.\n\n\n\n\n.params                  = []  \n.self_updating_variables = [] \n.sub_modules             = OrderedDict()\n.name                    = name\n.work_mode               = work_mode\n\n\n\n\n\n\nparams\n: contains all the parameters which should be updated by optimizer (submodule excluded)\n\n\nself_updating_variables\n: contains all the parameters which are updated by user specified expression (submoduluded)\n\n\nsub_modules\n: contains all the sub-modules\n\n\n\n\n.register_param(x, shape=None, name=None)\n.register_self_updating_variable(x, shape=None, name=None)\n\n\n\n\nRegister and possibly initialize a parameter tensor. Parameters to be updated by optimizer should be registered with \nregister_param()\n meanwhile parameters self-updated should be registerd with \nregister_self_updating_variable()\n\n\n\n\nx\n: Theano shared variable, expression, numpy array or callable. Initial value, expression or initializer for this parameter.\n\n\nshape\n: tuple of int, optional. A tuple of integers representing the desired shape of the parameter tensor.\n\n\nname\n: str, optional. A descriptive name for the parameter variable. If not specified, it'll be auto-named as [1]_[2]@[3], in which 1 is the variable instance name, 2 is the class name, and 3 is the module instance name\n\n\n\n\n.collect_params(include=None, exclude=None, include_self=True)\n\n\n\n\nCollect parameters to be updated by optimizer.\n\n\n\n\ninclude\n: sub-module keys, means which sub-module to include\n\n\nexclude\n: sub-module keys, means which sub-module to exclude\n\n\ninclude_self\n: whether include \nself.params\n\n\nreturn\n: list of parameters, in the same order of sub-modules\n\n\n\n\n.collect_self_updates(include=None, exclude=None, include_self=True)\n\n\n\n\nCollect all \nupdate\n from self_updating_variables.\n\n\n\n\ninclude\n: sub-module keys, means which sub-module to include\n\n\nexclude\n: sub-module keys, means which sub-module to exclude\n\n\ninclude_self\n: whether include \nself.self_updating_variables\n\n\nreturn\n: update dict, in the same order of sub-modules\n\n\n\n\n.get_weights()\n\n\n\n\nCollect all module weights (including submodules)\n\n\n\n\nreturn\n: list of tuples with format [variable.value, variable.name]\n\n\n\n\n.set_weights(module_weights, check_name='ignore')\n\n\n\n\nSet module weights by default order (same order with \n.get_weights()\n)\n\n\n\n\nmodule_weights\n: same with the return of \n.get_weights()\n\n\ncheck_name\n: \nignore\n|\nwarn\n|\nraise\n. What to do if a weight's name does not match its corresponding variable's name.\n\n\n\n\n.set_weights_by_name(module_weights, unmatched='ignore')\n\n\n\n\nSet module weights by matching name.\n\n\n\n\nmodule_weights\n: same with the return of \n.get_weights()\n\n\nunmatched\n:  \nignore\n|\nwarn\n|\nraise\n. What to do if there remain weights or module variables unmatched.\n\n\n\n\n\n\nDropout\n\n\nSets values to zero with probability \np\n\n\nclass Dropout(seed=None, name=None)\n\n\n\n\n\n\nseed\n: the random seed (integer) for initialization, optional\n\n\n\n\n.forward(input, p=0.5, shared_axes=(), rescale=True)\n\n\n\n\n\n\np\n: \ufb02oat or scalar tensor. The probability of setting a value to zero\n\n\nshared_axes\n: tuple of int. Axes to share the dropout mask over. By default, each value can be dropped individually. \nshared_axes\n=(0,) uses the same mask across the batch. \nshared_axes\n=(2, 3) uses the same mask across the spatial dimensions of 2D feature maps.\n\n\nrescale\n: bool. If True (the default), scale the input by 1 / (1 - \np\n) when dropout is enabled, to keep the expected output mean the same.\n\n\n\n\n.predict( input, *args, **kwargs)\n\n\n\n\ndummy interface, does nothing but returns the input unchanged.\n\n\nNote: Theano uses \nself_update\n mechanism to implement pseudo randomness, so to use \nDropout\n class, the followings are recommened:\n\n\n\n\n(1) define different instance for each droput layer\n\n\n(2) compiling function with \nno_default_updates=False\n\n\n\n\n\n\nGRU\n\n\nGated Recurrent Unit RNN.\n\n\nclass GRU(input_dims, hidden_dim, initializer=init.Normal(0.1), grad_clipping=0, \n          hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None)\n\n\n\n\n\n\ninput_dims\n: integer or list of integers. If scalar, input dimension = \ninput_dims\n; if list of integers, input dimension = sum(\ninput_dims\n), and GRU\u2019s parameter \nW_in\n will be initialized unevenly by integers specified in input_dims\n\n\nhidden_dim\n: dimension of hidden units, also the output dimension\n\n\ngrad_clipping\n: float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping\n\n\nhidden_activation\n: nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use \ntanh\n as default.\n\n\nlearn_ini\n: whether learn initial state\n\n\ntruncate_gradient\n: if not -1, BPTT will be used, gradient back-propagation will be performed at most \ntruncate_gradient\n steps\n\n\n\n\n.forward(seq_input, h_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False)\n\n\n\n\n\n\nseq_input\n: tensor with shape (T, B, D) in which D is the input dimension\n\n\nh_ini\n: initialization of hidden cell, (B, hidden_dim)\n\n\nseq_mask\n: mask for \nseq_input\n\n\nbackward\n: bool. Whether scan in backward direction\n\n\nonly_return_final\n: bool. If \nTrue\n, only return the \ufb01nal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory.\n\n\nreturn_final_state\n: If \nTrue\n, the final state of \nhidden\n and \ncell\n will be returned, both (B, hidden_dim)\n\n\n\n\n.predict = .forward\n\n\n\n\n\n\nLSTM\n\n\nLong Short-Term Memory RNN\n\n\nclass LSTM( input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, \n            hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None)\n\n\n\n\n\n\ninput_dims\n: integer or list of integers. If scalar, input dimension = \ninput_dims\n; if list of integers, input dimension = sum(\ninput_dims\n), and LSTM\u2019s parameter \nW_in\n will be initialized unevenly by integers specified in input_dims\n\n\nhidden_dim\n: dimension of hidden units, also the output dimension\n\n\npeephole\n: bool. Whether add peephole connection.\n\n\ngrad_clipping\n: float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping\n\n\nhidden_activation\n: nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use \ntanh\n as default.\n\n\nlearn_ini\n: whether learn initial state\n\n\ntruncate_gradient\n: if not -1, BPTT will be used, gradient back-propagation will be performed at most \ntruncate_gradient\n steps\n\n\n\n\n.forward(seq_input, h_ini=None, c_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False)\n\n\n\n\n\n\nseq_input\n: tensor with shape (T, B, D) in which D is the input dimension\n\n\nh_ini\n: initialization of hidden state, (B, hidden_dim)\n\n\nc_ini\n: initialization of cell state, (B, hidden_dim)\n\n\nseq_mask\n: mask for seq_input\n\n\nbackward\n: bool. Whether scan in backward direction\n\n\nonly_return_final\n: bool. If \nTrue\n, only return the \ufb01nal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory.\n\n\nreturn_final_state\n: If \nTrue\n, the final state of \nhidden\n and \ncell\n will be returned, both (B, hidden_dim)\n\n\n\n\n.predict = .forward\n\n\n\n\n\n\nGRUCell\n\n\nGated Recurrent Unit RNN Cell\n\n\nclass GRUCell(input_dims, hidden_dim, initializer=init.Normal(0.1), grad_clipping=0, \n              hidden_activation=tanh, name=None)\n\n\n\n\n\n\ninput_dims\n: integer or list of integers. If scalar, input dimension = \ninput_dims\n; if list of integers, input dimension = sum(\ninput_dims\n), and GRUCell\u2019s parameter \nW_in\n will be initialized unevenly by integers specified in input_dims\n\n\nhidden_dim\n: dimension of hidden units, also the output dimension\n\n\ngrad_clipping\n: float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping\n\n\nhidden_activation\n: nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use \ntanh\n as default\n\n\n\n\n.forward(input, h_pre, mask=None)\n\n\n\n\n\n\ninput\n: tensor with shape (B, D) in which D is the input dimension\n\n\nh_pre\n: initialization of hidden cell, (B, hidden_dim)\n\n\nmask\n: mask for \ninput\n\n\n\n\n.predict = .forward\n\n\n\n\n\n\nLSTMCell\n\n\nLong Short-Term Memory RNN Cell\n\n\nclass LSTMCell(input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, \n               hidden_activation=tanh, name=None)\n\n\n\n\n\n\ninput_dims\n: integer or list of integers. If scalar, input dimension = \ninput_dims\n; if list of integers, input dimension = sum(\ninput_dims\n), and LSTM\u2019s parameter \nW_in\n will be initialized unevenly by integers specified in input_dims\n\n\nhidden_dim\n: dimension of hidden units, also the output dimension\n\n\npeephole\n: bool. Whether add peephole connection.\n\n\ngrad_clipping\n: float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping\n\n\nhidden_activation\n: nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use \ntanh\n as default.\n\n\n\n\n.forward(input, h_pre, c_pre, mask=None)\n\n\n\n\n\n\ninput\n: tensor with shape (B, D) in which D is the input dimension\n\n\nh_pre\n: initialization of hidden state, (B, hidden_dim)\n\n\nc_pre\n: initialization of cell state, (B, hidden_dim)\n\n\nmask\n: mask for \ninput\n\n\n\n\n.predict = .forward\n\n\n\n\n\n\nConv2D\n\n\nConvolution 2D\n\n\nclass Conv2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), pad='valid', \n             dilation=(1,1), num_groups=1, W=init.GlorotUniform(), b=init.Constant(0.), \n             flip_filters=True, convOP=tensor.nnet.conv2d, input_shape=(None,None), untie_bias=False, name=None)\n\n\n\n\n\n\ninput_channels\n: int. Input shape of Conv2D module is (B, input_channels, H_in, W_in)\n\n\nout_channels\n: int. Output shape of Conv2D module is (B output_channels, H_out, W_out)\n\n\nkernel_size\n: int scalar or tuple of int. Convolution kernel size\n\n\nstride\n: Factor by which to subsample the output\n\n\npad\n: \nsame\n/\nvalid\n/\nfull\n or 2-element tuple of int. Control image border padding.\n\n\ndilation\n: factor by which to subsample (stride) the input.\n\n\nnum_groups\n: Divides the image, kernel and output tensors into num_groups separate groups. Each which carry out convolutions separately\n\n\nW\n: initialization of filter bank, shape = (out_channels, in_channels, kernel_size[0], kernel_size[1])\n\n\nb\n: initialization of convolution bias, shape = (out_channels,) if untie_bias is False; otherwise shape = (out_channels, H_out, W_out)\n\n\nflip_filters\n: If \nTrue\n, will flip the filter rows and columns before sliding them over the input. This operation is normally referred to as a convolution, and this is the default. If \nFalse\n, the filters are not flipped and the operation is referred to as a cross-correlation.\n\n\ninput_shape\n: optional, (H_in, W_in)\n\n\nuntie_bias\n: If \nFalse\n, the module will have a bias parameter for each channel, which is shared across all positions in this channel. As a result, the b attribute will be a vector (1D). If \nTrue\n, the module will have separate bias parameters for each position in each channel. As a result, the b attribute will be a 3D tensor.\n\n\n\n\n\n\nConvTransposed2D\n\n\nTransposed convolution 2D. Also known as fractionally-strided convolution or deconvolution (although it is not an actual deconvolution operation)\n\n\nclass ConvTransposed2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), pad='valid', \n                       dilation=(1,1), num_groups=1, W=init.GlorotUniform(), b=init.Constant(0.), \n                       flip_filters=True, input_shape=(None,None), untie_bias=False, name=None)\n\n\n\n\nAll the parameters have the same meanings with \nConv2D\n module. In fact, the transposed convolution is equal to upsampling the input then doing conventional convolution. However, for efficiency purpose, here the transposed convolution is implemented via Theano\u2019s \nAbstractConv2d_gradInputs\n as what is done in Lasagne.\n\n\n\n\nDense\n\n\nFully connected network, also known as affine transform. Apply affine transform \nWx+b\n to the last dimension of input.\n\nThe input of \nDense\n can have any dimensions, and note that we do not apply any activation to its output by default\n\n\nclass Dense(input_dims, output_dim, W=init.GlorotUniform(), b=init.Constant(0.), name=None)\n\n\n\n\n\n\ninput_dims\n: integer or list of integers. If scalar, input dimension = input_dims; if list of integers, input dimension = sum(input_dims), and Dense\u2019s parameter \nW\n will be initialized unevenly by integers specified in input_dims\n\n\noutput_dim\n: output dimension\n\n\nW\n, \nb\n: parameter initialization\n\n\n\n\n\n\nEmbedding\n\n\nWord/character embedding module.\n\n\nclass Embedding(num_embeddings, embedding_dim, W=init.Normal(), name=None)\n\n\n\n\n\n\nnum_embeddings\n: the Number of different embeddings\n\n\nembedding_dim\n: output embedding vector dimension\n\n\n\n\n\n\nBatchNorm\n\n\nBatch normalization module.\n\n\nclass BatchNorm(input_shape=None, axes='auto', eps=1e-4, alpha=0.1, beta=init.Constant(0), gamma=init.Constant(1), \n                mean=init.Constant(0), inv_std=init.Constant(1), mode='high_mem', name=None)\n\n\n\n\n\n\ninput_shape\n: Tuple or list of ints or tensor variables. Input shape of \nBatchNorm\n module, including batch dimension. \n\n\naxes\n: \nauto\n or tuple of int. The axis or axes to normalize over. If \nauto\n (the default), normalize over all axes except for the second: this will normalize over the minibatch dimension for dense layers, and additionally over all spatial dimensions for convolutional layers.\n\n\neps\n: Small constant \ud835\udf16 added to the variance before taking the square root and dividing by it, to avoid numerical problems\n\n\nalpha\n: Coefficient for the exponential moving average of batch-wise means and standard deviations computed during training; the closer to one, the more it will depend on the last batches seen\n\n\nmode\n: \nlow_mem\n or \nhigh_mem\n. Specify which batch normalization implementation that will be used. As no intermediate representations are stored for the back-propagation, \nlow_mem\n implementation lower the memory usage, however, it is 5-10% slower than \nhigh_mem\n implementation. Note that 5-10% computation time difference compare the batch normalization operation only, time difference between implementation is likely to be less important on the full model fprop/bprop.\n\n\n\n\n.forward(input, use_input_mean=True)\n\n\n\n\n\n\nuse_input_mean\n: default, use mean & std of input batch for normalization; if \nFalse\n, \nself.mean\n and \nself.std\n will be used for normalization. The reason that input mean is used during training is because at the early training stage, \nBatchNorm\n's \nself.mean\n is far from the expected mean value and can be detrimental for network convergence. It's recommended to use input mean for early stage training; after that, you can switch to \nBatchNorm\n's \nself.mean\n for training & inference consistency.\n\n\n\n\n\n\nCenter\n\n\nEstimate class centers by moving averaging.\n\n\nclass Center(feature_dim, center_num, alpha=0.1, center=init.GlorotUniform(), name=None)\n\n\n\n\n\n\nfeature_dim\n: feature dimension \n\n\ncenter_num\n: class center number\n\n\nalpha\n: moving averaging coefficient, the closer to one, the more it will depend on the last batches seen\n\n\ncenter\n: initialization of class centers, should be in shape of \n(center_num, feature_dim)\n\n\n\n\n.forward(features, labels)\n\n\n\n\n\n\nfeatures\n: batch features, from which the class centers will be estimated\n\n\nlabels\n: \nfeatures\n's corresponding class labels\n\n\nreturn\n: centers estimated\n\n\n\n\n.predict()\n\n\n\n\n\n\nreturn\n: centers stored\n\n\n\n\n\n\nChainCRF\n\n\nLinear chain CRF layer for sequence labeling.\n\n\nclass ChainCRF(state_num, transitions=init.GlorotUniform(), p_scale=1.0, l1_regularization=0.001, \n               state_pad=True, transition_matrix_normalization=True,  name=None)\n\n\n\n\n\n\nstate_num\n: number of hidden states. If \nstate_pad\n is \nTrue\n, then the actual state number inside CRF will be \nstate_num + 2\n.\n\n\ntransitions\n: initialization of transition matrix, in shape of \n(state_num+2, state_num+2)\n if \nstate_pad\n is \nTrue\n, else \n(state_num, state_num)\n\n\np_scale\n: probability scale factor. The input of this module will be multiplied by this factor.\n\n\nl1_regularization\n: L1 regularization coefficient for \ntransition\n matrix\n\n\nstate_pad\n: whether do state padding. CRF requires two additional dummy states, i.e., \n<bos>\n and \n<eos>\n (beginning and endding of sequence). The \nChainCRF\n module can pad the state automatically with these two dummy states, or you can incorporate these two states in module input. In the latter case, set \nstate_pad\n to \nFalse\n.\n\n\ntransition_matrix_normalization\n: whether do row-wise normalization of transition matrix. You may expect that each row of the \ntransition\n matrix should sum to 1.0, and to do this, set this flag to \nTrue\n.\n\n\n\n\n.forward(x, y)\n\n\n\n\nCompute CRF loss\n\n\n\n\nx\n: output from previous RNN layer, in shape of (B, T, N)\n\n\ny\n: tag ground truth, in shape of (B, T), int32\n\n\nreturn\n: loss in shape of (B,) if \nl1_regularization\n disabled, else in shape of (1,)\n\n\n\n\n.predict(x)\n\n\n\n\nCRF Viterbi decoding\n\n\n\n\nx\n: output from previous RNN layer, in shape of (B, T, N)\n\n\nreturn\n: decoded sequence",
            "title": "dandelion.module"
        },
        {
            "location": "/dandelion_module/#module",
            "text": "Root class of all network modules, you'd always subclass this for a new module  class Module(name=None, work_mode='inference')   name : module name, optional. If you don't specify the module name, it will be auto-named if this module is a sub-module of another module.  work_mode : working mode, optional. Only used for the unified calling interface, check \"Tutorial I\" for detailed explanation.   .params                  = []  \n.self_updating_variables = [] \n.sub_modules             = OrderedDict()\n.name                    = name\n.work_mode               = work_mode   params : contains all the parameters which should be updated by optimizer (submodule excluded)  self_updating_variables : contains all the parameters which are updated by user specified expression (submoduluded)  sub_modules : contains all the sub-modules   .register_param(x, shape=None, name=None)\n.register_self_updating_variable(x, shape=None, name=None)  Register and possibly initialize a parameter tensor. Parameters to be updated by optimizer should be registered with  register_param()  meanwhile parameters self-updated should be registerd with  register_self_updating_variable()   x : Theano shared variable, expression, numpy array or callable. Initial value, expression or initializer for this parameter.  shape : tuple of int, optional. A tuple of integers representing the desired shape of the parameter tensor.  name : str, optional. A descriptive name for the parameter variable. If not specified, it'll be auto-named as [1]_[2]@[3], in which 1 is the variable instance name, 2 is the class name, and 3 is the module instance name   .collect_params(include=None, exclude=None, include_self=True)  Collect parameters to be updated by optimizer.   include : sub-module keys, means which sub-module to include  exclude : sub-module keys, means which sub-module to exclude  include_self : whether include  self.params  return : list of parameters, in the same order of sub-modules   .collect_self_updates(include=None, exclude=None, include_self=True)  Collect all  update  from self_updating_variables.   include : sub-module keys, means which sub-module to include  exclude : sub-module keys, means which sub-module to exclude  include_self : whether include  self.self_updating_variables  return : update dict, in the same order of sub-modules   .get_weights()  Collect all module weights (including submodules)   return : list of tuples with format [variable.value, variable.name]   .set_weights(module_weights, check_name='ignore')  Set module weights by default order (same order with  .get_weights() )   module_weights : same with the return of  .get_weights()  check_name :  ignore | warn | raise . What to do if a weight's name does not match its corresponding variable's name.   .set_weights_by_name(module_weights, unmatched='ignore')  Set module weights by matching name.   module_weights : same with the return of  .get_weights()  unmatched :   ignore | warn | raise . What to do if there remain weights or module variables unmatched.",
            "title": "Module"
        },
        {
            "location": "/dandelion_module/#dropout",
            "text": "Sets values to zero with probability  p  class Dropout(seed=None, name=None)   seed : the random seed (integer) for initialization, optional   .forward(input, p=0.5, shared_axes=(), rescale=True)   p : \ufb02oat or scalar tensor. The probability of setting a value to zero  shared_axes : tuple of int. Axes to share the dropout mask over. By default, each value can be dropped individually.  shared_axes =(0,) uses the same mask across the batch.  shared_axes =(2, 3) uses the same mask across the spatial dimensions of 2D feature maps.  rescale : bool. If True (the default), scale the input by 1 / (1 -  p ) when dropout is enabled, to keep the expected output mean the same.   .predict( input, *args, **kwargs)  dummy interface, does nothing but returns the input unchanged.  Note: Theano uses  self_update  mechanism to implement pseudo randomness, so to use  Dropout  class, the followings are recommened:   (1) define different instance for each droput layer  (2) compiling function with  no_default_updates=False",
            "title": "Dropout"
        },
        {
            "location": "/dandelion_module/#gru",
            "text": "Gated Recurrent Unit RNN.  class GRU(input_dims, hidden_dim, initializer=init.Normal(0.1), grad_clipping=0, \n          hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None)   input_dims : integer or list of integers. If scalar, input dimension =  input_dims ; if list of integers, input dimension = sum( input_dims ), and GRU\u2019s parameter  W_in  will be initialized unevenly by integers specified in input_dims  hidden_dim : dimension of hidden units, also the output dimension  grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping  hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use  tanh  as default.  learn_ini : whether learn initial state  truncate_gradient : if not -1, BPTT will be used, gradient back-propagation will be performed at most  truncate_gradient  steps   .forward(seq_input, h_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False)   seq_input : tensor with shape (T, B, D) in which D is the input dimension  h_ini : initialization of hidden cell, (B, hidden_dim)  seq_mask : mask for  seq_input  backward : bool. Whether scan in backward direction  only_return_final : bool. If  True , only return the \ufb01nal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory.  return_final_state : If  True , the final state of  hidden  and  cell  will be returned, both (B, hidden_dim)   .predict = .forward",
            "title": "GRU"
        },
        {
            "location": "/dandelion_module/#lstm",
            "text": "Long Short-Term Memory RNN  class LSTM( input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, \n            hidden_activation=tanh, learn_ini=False, truncate_gradient=-1, name=None)   input_dims : integer or list of integers. If scalar, input dimension =  input_dims ; if list of integers, input dimension = sum( input_dims ), and LSTM\u2019s parameter  W_in  will be initialized unevenly by integers specified in input_dims  hidden_dim : dimension of hidden units, also the output dimension  peephole : bool. Whether add peephole connection.  grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping  hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use  tanh  as default.  learn_ini : whether learn initial state  truncate_gradient : if not -1, BPTT will be used, gradient back-propagation will be performed at most  truncate_gradient  steps   .forward(seq_input, h_ini=None, c_ini=None, seq_mask=None, backward=False, only_return_final=False, return_final_state=False)   seq_input : tensor with shape (T, B, D) in which D is the input dimension  h_ini : initialization of hidden state, (B, hidden_dim)  c_ini : initialization of cell state, (B, hidden_dim)  seq_mask : mask for seq_input  backward : bool. Whether scan in backward direction  only_return_final : bool. If  True , only return the \ufb01nal sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization which saves memory.  return_final_state : If  True , the final state of  hidden  and  cell  will be returned, both (B, hidden_dim)   .predict = .forward",
            "title": "LSTM"
        },
        {
            "location": "/dandelion_module/#grucell",
            "text": "Gated Recurrent Unit RNN Cell  class GRUCell(input_dims, hidden_dim, initializer=init.Normal(0.1), grad_clipping=0, \n              hidden_activation=tanh, name=None)   input_dims : integer or list of integers. If scalar, input dimension =  input_dims ; if list of integers, input dimension = sum( input_dims ), and GRUCell\u2019s parameter  W_in  will be initialized unevenly by integers specified in input_dims  hidden_dim : dimension of hidden units, also the output dimension  grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping  hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use  tanh  as default   .forward(input, h_pre, mask=None)   input : tensor with shape (B, D) in which D is the input dimension  h_pre : initialization of hidden cell, (B, hidden_dim)  mask : mask for  input   .predict = .forward",
            "title": "GRUCell"
        },
        {
            "location": "/dandelion_module/#lstmcell",
            "text": "Long Short-Term Memory RNN Cell  class LSTMCell(input_dims, hidden_dim, peephole=True, initializer=init.Normal(0.1), grad_clipping=0, \n               hidden_activation=tanh, name=None)   input_dims : integer or list of integers. If scalar, input dimension =  input_dims ; if list of integers, input dimension = sum( input_dims ), and LSTM\u2019s parameter  W_in  will be initialized unevenly by integers specified in input_dims  hidden_dim : dimension of hidden units, also the output dimension  peephole : bool. Whether add peephole connection.  grad_clipping : float. Hard clip the gradients at each time step. Only the gradient values above this threshold are clipped to the threshold. This is done during backprop. Some works report that using grad_normalization is better than grad_clipping  hidden_activation : nonlinearity applied to hidden variable, i.e., h = hidden_activation(cell). It's recommended to use  tanh  as default.   .forward(input, h_pre, c_pre, mask=None)   input : tensor with shape (B, D) in which D is the input dimension  h_pre : initialization of hidden state, (B, hidden_dim)  c_pre : initialization of cell state, (B, hidden_dim)  mask : mask for  input   .predict = .forward",
            "title": "LSTMCell"
        },
        {
            "location": "/dandelion_module/#conv2d",
            "text": "Convolution 2D  class Conv2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), pad='valid', \n             dilation=(1,1), num_groups=1, W=init.GlorotUniform(), b=init.Constant(0.), \n             flip_filters=True, convOP=tensor.nnet.conv2d, input_shape=(None,None), untie_bias=False, name=None)   input_channels : int. Input shape of Conv2D module is (B, input_channels, H_in, W_in)  out_channels : int. Output shape of Conv2D module is (B output_channels, H_out, W_out)  kernel_size : int scalar or tuple of int. Convolution kernel size  stride : Factor by which to subsample the output  pad :  same / valid / full  or 2-element tuple of int. Control image border padding.  dilation : factor by which to subsample (stride) the input.  num_groups : Divides the image, kernel and output tensors into num_groups separate groups. Each which carry out convolutions separately  W : initialization of filter bank, shape = (out_channels, in_channels, kernel_size[0], kernel_size[1])  b : initialization of convolution bias, shape = (out_channels,) if untie_bias is False; otherwise shape = (out_channels, H_out, W_out)  flip_filters : If  True , will flip the filter rows and columns before sliding them over the input. This operation is normally referred to as a convolution, and this is the default. If  False , the filters are not flipped and the operation is referred to as a cross-correlation.  input_shape : optional, (H_in, W_in)  untie_bias : If  False , the module will have a bias parameter for each channel, which is shared across all positions in this channel. As a result, the b attribute will be a vector (1D). If  True , the module will have separate bias parameters for each position in each channel. As a result, the b attribute will be a 3D tensor.",
            "title": "Conv2D"
        },
        {
            "location": "/dandelion_module/#convtransposed2d",
            "text": "Transposed convolution 2D. Also known as fractionally-strided convolution or deconvolution (although it is not an actual deconvolution operation)  class ConvTransposed2D(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), pad='valid', \n                       dilation=(1,1), num_groups=1, W=init.GlorotUniform(), b=init.Constant(0.), \n                       flip_filters=True, input_shape=(None,None), untie_bias=False, name=None)  All the parameters have the same meanings with  Conv2D  module. In fact, the transposed convolution is equal to upsampling the input then doing conventional convolution. However, for efficiency purpose, here the transposed convolution is implemented via Theano\u2019s  AbstractConv2d_gradInputs  as what is done in Lasagne.",
            "title": "ConvTransposed2D"
        },
        {
            "location": "/dandelion_module/#dense",
            "text": "Fully connected network, also known as affine transform. Apply affine transform  Wx+b  to the last dimension of input. \nThe input of  Dense  can have any dimensions, and note that we do not apply any activation to its output by default  class Dense(input_dims, output_dim, W=init.GlorotUniform(), b=init.Constant(0.), name=None)   input_dims : integer or list of integers. If scalar, input dimension = input_dims; if list of integers, input dimension = sum(input_dims), and Dense\u2019s parameter  W  will be initialized unevenly by integers specified in input_dims  output_dim : output dimension  W ,  b : parameter initialization",
            "title": "Dense"
        },
        {
            "location": "/dandelion_module/#embedding",
            "text": "Word/character embedding module.  class Embedding(num_embeddings, embedding_dim, W=init.Normal(), name=None)   num_embeddings : the Number of different embeddings  embedding_dim : output embedding vector dimension",
            "title": "Embedding"
        },
        {
            "location": "/dandelion_module/#batchnorm",
            "text": "Batch normalization module.  class BatchNorm(input_shape=None, axes='auto', eps=1e-4, alpha=0.1, beta=init.Constant(0), gamma=init.Constant(1), \n                mean=init.Constant(0), inv_std=init.Constant(1), mode='high_mem', name=None)   input_shape : Tuple or list of ints or tensor variables. Input shape of  BatchNorm  module, including batch dimension.   axes :  auto  or tuple of int. The axis or axes to normalize over. If  auto  (the default), normalize over all axes except for the second: this will normalize over the minibatch dimension for dense layers, and additionally over all spatial dimensions for convolutional layers.  eps : Small constant \ud835\udf16 added to the variance before taking the square root and dividing by it, to avoid numerical problems  alpha : Coefficient for the exponential moving average of batch-wise means and standard deviations computed during training; the closer to one, the more it will depend on the last batches seen  mode :  low_mem  or  high_mem . Specify which batch normalization implementation that will be used. As no intermediate representations are stored for the back-propagation,  low_mem  implementation lower the memory usage, however, it is 5-10% slower than  high_mem  implementation. Note that 5-10% computation time difference compare the batch normalization operation only, time difference between implementation is likely to be less important on the full model fprop/bprop.   .forward(input, use_input_mean=True)   use_input_mean : default, use mean & std of input batch for normalization; if  False ,  self.mean  and  self.std  will be used for normalization. The reason that input mean is used during training is because at the early training stage,  BatchNorm 's  self.mean  is far from the expected mean value and can be detrimental for network convergence. It's recommended to use input mean for early stage training; after that, you can switch to  BatchNorm 's  self.mean  for training & inference consistency.",
            "title": "BatchNorm"
        },
        {
            "location": "/dandelion_module/#center",
            "text": "Estimate class centers by moving averaging.  class Center(feature_dim, center_num, alpha=0.1, center=init.GlorotUniform(), name=None)   feature_dim : feature dimension   center_num : class center number  alpha : moving averaging coefficient, the closer to one, the more it will depend on the last batches seen  center : initialization of class centers, should be in shape of  (center_num, feature_dim)   .forward(features, labels)   features : batch features, from which the class centers will be estimated  labels :  features 's corresponding class labels  return : centers estimated   .predict()   return : centers stored",
            "title": "Center"
        },
        {
            "location": "/dandelion_module/#chaincrf",
            "text": "Linear chain CRF layer for sequence labeling.  class ChainCRF(state_num, transitions=init.GlorotUniform(), p_scale=1.0, l1_regularization=0.001, \n               state_pad=True, transition_matrix_normalization=True,  name=None)   state_num : number of hidden states. If  state_pad  is  True , then the actual state number inside CRF will be  state_num + 2 .  transitions : initialization of transition matrix, in shape of  (state_num+2, state_num+2)  if  state_pad  is  True , else  (state_num, state_num)  p_scale : probability scale factor. The input of this module will be multiplied by this factor.  l1_regularization : L1 regularization coefficient for  transition  matrix  state_pad : whether do state padding. CRF requires two additional dummy states, i.e.,  <bos>  and  <eos>  (beginning and endding of sequence). The  ChainCRF  module can pad the state automatically with these two dummy states, or you can incorporate these two states in module input. In the latter case, set  state_pad  to  False .  transition_matrix_normalization : whether do row-wise normalization of transition matrix. You may expect that each row of the  transition  matrix should sum to 1.0, and to do this, set this flag to  True .   .forward(x, y)  Compute CRF loss   x : output from previous RNN layer, in shape of (B, T, N)  y : tag ground truth, in shape of (B, T), int32  return : loss in shape of (B,) if  l1_regularization  disabled, else in shape of (1,)   .predict(x)  CRF Viterbi decoding   x : output from previous RNN layer, in shape of (B, T, N)  return : decoded sequence",
            "title": "ChainCRF"
        },
        {
            "location": "/dandelion_functional/",
            "text": "pool_1d\n\n\nPooling 1 dimension along the given axis, support for any dimensional input.\n\n\npool_1d(x, ws=2, ignore_border=True, stride=None, pad=0, mode='max', axis=-1)\n\n\n\n\n\n\nws\n: scalar int. Factor by which to downsample the input\n\n\nignore_border\n: bool. When \nTrue\n, dimension size=5 with \nws\n=2 will generate a dimension size=2 output. 3 otherwise.\n\n\nstride\n: scalar int. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions), eg: \nstride\n=1 will shifts over one row for every iteration.\n\n\npad\n: pad zeros to extend beyond border of the input\n\n\nmode\n: {\nmax\n, \nsum\n, \naverage_inc_pad\n, \naverage_exc_pad\n}. Operation executed on each window. \nmax\n and \nsum\n always exclude the padding in the computation. \naverage\n gives you the choice to include or exclude it.\n\n\naxis\n: scalar int. Specify along which axis the pooling will be done\n\n\n\n\n\n\npool_2d\n\n\nPooling 2 dimension along the last 2 dimensions of input, support for any dimensional input with \nndim\n>=2.\n\n\npool_2d(x, ws=(2,2), ignore_border=True, stride=None, pad=(0,0), mode='max')\n\n\n\n\n\n\nws\n: scalar tuple. Factor by which to downsample the input\n\n\nignore_border\n: bool. When \nTrue\n, (5,5) input with \nws\n=(2,2) will generate a (2,2) output. (3,3) otherwise.\n\n\nstride\n: scalar tuple. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions), eg: \nstride\n=(1,1) will shifts over one row and one column for every iteration.\n\n\npad\n: pad zeros to extend beyond border of the input\n\n\nmode\n: {\nmax\n, \nsum\n, \naverage_inc_pad\n, \naverage_exc_pad\n}. Operation executed on each window. \nmax\n and \nsum\n always exclude the padding in the computation. \naverage\n gives you the choice to include or exclude it.\n\n\n\n\n\n\npool_3d\n\n\nPooling 3 dimension along the last 3 dimensions of input, support for any dimensional input with \nndim\n>=3.\n\n\npool_3d(x, ws=(2,2,2), ignore_border=True, stride=None, pad=(0,0,0), mode='max')\n\n\n\n\n\n\nws\n: scalar tuple. Factor by which to downsample the input\n\n\nignore_border\n: bool. When \nTrue\n, (5,5,5) input with \nws\n=(2,2,2) will generate a (2,2,2) output. (3,3,3) otherwise.\n\n\nstride\n: scalar tuple. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions).\n\n\npad\n: pad zeros to extend beyond border of the input\n\n\nmode\n: {\nmax\n, \nsum\n, \naverage_inc_pad\n, \naverage_exc_pad\n}. Operation executed on each window. \nmax\n and \nsum\n always exclude the padding in the computation. \naverage\n gives you the choice to include or exclude it.\n\n\n\n\n\n\nalign_crop\n\n\nAlign a list of tensors at each axis by specified rules and crop them to make axis concatenation possible.\n\n\nalign_crop(tensor_list, cropping)\n\n\n\n\n\n\ntensor_list\n: list of tensors to be processed, they much have the same \nndim\ns.\n\n\ncropping\n: list of cropping rules for each dimension. Acceptable rules include {\nNone\n|\nlower\n|\nupper\n|\ncenter\n}. \n\n\nNone\n: this axis is not cropped, tensors are unchanged in this axis\n\n\nlower\n: tensors are cropped choosing the lower portion in this axis as \na[:crop_size, ...]\n\n\nupper\n: tensors are cropped choosing the upper portion in this axis as \na[-crop_size:, ...]\n\n\ncenter\n: tensors are cropped choosing the central portion in this axis as \na[offset:offset+crop_size, ...]\n where \noffset = (a.shape[0]-crop_size)//2)",
            "title": "dandelion.functional"
        },
        {
            "location": "/dandelion_functional/#pool_1d",
            "text": "Pooling 1 dimension along the given axis, support for any dimensional input.  pool_1d(x, ws=2, ignore_border=True, stride=None, pad=0, mode='max', axis=-1)   ws : scalar int. Factor by which to downsample the input  ignore_border : bool. When  True , dimension size=5 with  ws =2 will generate a dimension size=2 output. 3 otherwise.  stride : scalar int. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions), eg:  stride =1 will shifts over one row for every iteration.  pad : pad zeros to extend beyond border of the input  mode : { max ,  sum ,  average_inc_pad ,  average_exc_pad }. Operation executed on each window.  max  and  sum  always exclude the padding in the computation.  average  gives you the choice to include or exclude it.  axis : scalar int. Specify along which axis the pooling will be done",
            "title": "pool_1d"
        },
        {
            "location": "/dandelion_functional/#pool_2d",
            "text": "Pooling 2 dimension along the last 2 dimensions of input, support for any dimensional input with  ndim >=2.  pool_2d(x, ws=(2,2), ignore_border=True, stride=None, pad=(0,0), mode='max')   ws : scalar tuple. Factor by which to downsample the input  ignore_border : bool. When  True , (5,5) input with  ws =(2,2) will generate a (2,2) output. (3,3) otherwise.  stride : scalar tuple. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions), eg:  stride =(1,1) will shifts over one row and one column for every iteration.  pad : pad zeros to extend beyond border of the input  mode : { max ,  sum ,  average_inc_pad ,  average_exc_pad }. Operation executed on each window.  max  and  sum  always exclude the padding in the computation.  average  gives you the choice to include or exclude it.",
            "title": "pool_2d"
        },
        {
            "location": "/dandelion_functional/#pool_3d",
            "text": "Pooling 3 dimension along the last 3 dimensions of input, support for any dimensional input with  ndim >=3.  pool_3d(x, ws=(2,2,2), ignore_border=True, stride=None, pad=(0,0,0), mode='max')   ws : scalar tuple. Factor by which to downsample the input  ignore_border : bool. When  True , (5,5,5) input with  ws =(2,2,2) will generate a (2,2,2) output. (3,3,3) otherwise.  stride : scalar tuple. The number of shifts over rows/cols to get the next pool region. If stride is None, it is considered equal to ws (no overlap on pooling regions).  pad : pad zeros to extend beyond border of the input  mode : { max ,  sum ,  average_inc_pad ,  average_exc_pad }. Operation executed on each window.  max  and  sum  always exclude the padding in the computation.  average  gives you the choice to include or exclude it.",
            "title": "pool_3d"
        },
        {
            "location": "/dandelion_functional/#align_crop",
            "text": "Align a list of tensors at each axis by specified rules and crop them to make axis concatenation possible.  align_crop(tensor_list, cropping)   tensor_list : list of tensors to be processed, they much have the same  ndim s.  cropping : list of cropping rules for each dimension. Acceptable rules include { None | lower | upper | center }.   None : this axis is not cropped, tensors are unchanged in this axis  lower : tensors are cropped choosing the lower portion in this axis as  a[:crop_size, ...]  upper : tensors are cropped choosing the upper portion in this axis as  a[-crop_size:, ...]  center : tensors are cropped choosing the central portion in this axis as  a[offset:offset+crop_size, ...]  where  offset = (a.shape[0]-crop_size)//2)",
            "title": "align_crop"
        },
        {
            "location": "/dandelion_objective/",
            "text": "Dandelion's \nobjective\n module is mostly inherited from \nLasagne\n except for the CTC (Connectionist Temporal Classification) objective. \n\n\nYou're recommended to refer to \nLasagne.objectives\n document\n for the following objectives:\n\n\n\n\nbinary_crossentropy\n\n\ncategorical_crossentropy\n\n\nsquared_error\n\n\nbinary_hinge_loss\n\n\nmulticlass_hinge_loss\n\n\nbinary_accuracy\n\n\ncategorical_accuracy\n\n\n\n\n\n\nctc_cost_logscale\n\n\nCTC cost calculated in \nlog\n scale. This CTC objective is written purely in Theano, so it runs on both Windows and Linux. Theano itself also has a \nwrapper\n for Baidu's \nwarp-ctc\n library, which requires separate install and only runs on Linux.\n\n\nctc_cost_logscale(seq, sm, seq_mask=None, sm_mask=None, blank_symbol=None, align='pre')\n\n\n\n\n\n\nseq\n: query sequence, shape of \n(L, B)\n, \nfloat32\n-typed\n\n\nsm\n: score matrix, shape of \n(T, C+1, B)\n, \nfloat32\n-typed\n\n\nseq_mask\n: mask for query sequence, shape of \n(L, B)\n, \nfloat32\n-typed\n\n\nsm_mask\n: mask for score matrix, shape of \n(T, B)\n, \nfloat32\n-typed\n\n\nblank_symbol\n: scalar, = \nC\n by default\n\n\nalign\n: string, {'pre'/'post'}, indicating how input samples are aligned in one batch\n\n\nreturn\n: negative log likelihood averaged over a batch\n\n\n\n\n\n\nctc_best_path_decode\n\n\nDecode the network output scorematrix by best-path-decoding scheme.\n\n\nctc_best_path_decode(Y, Y_mask=None, blank_symbol=None)\n\n\n\n\n\n\nY\n: output of a network, with shape \n(B, T, C+1)\n\n\nY_mask\n: mask of Y, with shape \n(B, T)\n\n\nreturn\n: result sequence of shape \n(T, B\n), and result sequence mask of shape \n(T, B)\n\n\n\n\n\n\nctc_CER\n\n\nCalculate the character error rate (CER) given ground truth \ntargetseq\n and CTC decoding output \nresultseq\n\n\nctc_CER(resultseq, targetseq, resultseq_mask=None, targetseq_mask=None)\n\n\n\n\n\n\nresultseq\n: CTC decoding output, with shape \n(T1, B)\n\n\ntargetseq\n: sequence ground truth, with shape \n(T2, B)\n\n\nreturn\n: tuple of \n(CER, TE, TG)\n, in which \nTE\n is the batch-wise total edit distance, \nTG\n is the batch-wise total ground truth sequence length, and \nCER\n equals to \nTE/TG",
            "title": "dandelion.objective"
        },
        {
            "location": "/dandelion_objective/#ctc_cost_logscale",
            "text": "CTC cost calculated in  log  scale. This CTC objective is written purely in Theano, so it runs on both Windows and Linux. Theano itself also has a  wrapper  for Baidu's  warp-ctc  library, which requires separate install and only runs on Linux.  ctc_cost_logscale(seq, sm, seq_mask=None, sm_mask=None, blank_symbol=None, align='pre')   seq : query sequence, shape of  (L, B) ,  float32 -typed  sm : score matrix, shape of  (T, C+1, B) ,  float32 -typed  seq_mask : mask for query sequence, shape of  (L, B) ,  float32 -typed  sm_mask : mask for score matrix, shape of  (T, B) ,  float32 -typed  blank_symbol : scalar, =  C  by default  align : string, {'pre'/'post'}, indicating how input samples are aligned in one batch  return : negative log likelihood averaged over a batch",
            "title": "ctc_cost_logscale"
        },
        {
            "location": "/dandelion_objective/#ctc_best_path_decode",
            "text": "Decode the network output scorematrix by best-path-decoding scheme.  ctc_best_path_decode(Y, Y_mask=None, blank_symbol=None)   Y : output of a network, with shape  (B, T, C+1)  Y_mask : mask of Y, with shape  (B, T)  return : result sequence of shape  (T, B ), and result sequence mask of shape  (T, B)",
            "title": "ctc_best_path_decode"
        },
        {
            "location": "/dandelion_objective/#ctc_cer",
            "text": "Calculate the character error rate (CER) given ground truth  targetseq  and CTC decoding output  resultseq  ctc_CER(resultseq, targetseq, resultseq_mask=None, targetseq_mask=None)   resultseq : CTC decoding output, with shape  (T1, B)  targetseq : sequence ground truth, with shape  (T2, B)  return : tuple of  (CER, TE, TG) , in which  TE  is the batch-wise total edit distance,  TG  is the batch-wise total ground truth sequence length, and  CER  equals to  TE/TG",
            "title": "ctc_CER"
        },
        {
            "location": "/dandelion_activation/",
            "text": "Dandelion's \nactivation\n module is mostly inherited from \nLasagne\n except for the \nsoftmax()\n activation. \n\n\nYou're recommended to refer to \nLasagne.nonlinearities\n document\n for the following activations:\n\n\n\n\nsigmoid\n\n\ntanh\n\n\nrelu\n\n\nsoftplus\n\n\nultra_fast_sigmoid\n\n\nScaledTanH\n\n\nleaky_rectify\n\n\nvery_leaky_rectify\n\n\nelu\n\n\nSELU\n\n\nlinear\n\n\nidentity\n\n\n\n\n\n\nsoftmax\n\n\nApply softmax to the last dimension of input \nx\n\n\nsoftmax(x)\n\n\n\n\n\n\nx\n: theano tensor of any shape",
            "title": "dandelion.activation"
        },
        {
            "location": "/dandelion_activation/#softmax",
            "text": "Apply softmax to the last dimension of input  x  softmax(x)   x : theano tensor of any shape",
            "title": "softmax"
        },
        {
            "location": "/dandelion_update/",
            "text": "Dandelion's \nupdate\n module is mostly inherited from \nLasagne\n, you're recommended to refer to \nLasagne.updates\n document\n for details.",
            "title": "dandelion.update"
        },
        {
            "location": "/dandelion_initialization/",
            "text": "Dandelion's \ninitialization\n module is mostly inherited from \nLasagne\n.\nYou're recommended to refer to \nLasagne.init\n document\n for the details.",
            "title": "dandelion.initialization"
        },
        {
            "location": "/dandelion_model/",
            "text": "U-net FCN\n\n\nImplementation of U-net FCN\n\n\nclass model_Unet(channel=1, im_height=128, im_width=128, Nclass=2, kernel_size=3, border_mode='same', base_n_filters=64, output_activation=softmax)\n\n\n\n\n\n\nchannel\n: input channel number\n\n\nNclass\n: output channel number\n\n\n\n\nThe model accepts input of shape in the order of (B, C, H, W), and outputs with shape in the order of (B, H, W, C).",
            "title": "dandelion.model"
        },
        {
            "location": "/dandelion_model/#u-net-fcn",
            "text": "Implementation of U-net FCN  class model_Unet(channel=1, im_height=128, im_width=128, Nclass=2, kernel_size=3, border_mode='same', base_n_filters=64, output_activation=softmax)   channel : input channel number  Nclass : output channel number   The model accepts input of shape in the order of (B, C, H, W), and outputs with shape in the order of (B, H, W, C).",
            "title": "U-net FCN"
        },
        {
            "location": "/dandelion_ext_CV/",
            "text": "imread\n\n\nRead image file and return as numpy \nndarray\n, using PILLOW as backend. Support for EXIF rotation specification.\n\n\nimread(f, flatten=False, dtype='float32')\n\n\n\n\n\n\nf\n: str or file object. The file name or file object to be read from.\n\n\nflatten\n: bool. If \nTrue\n, flattens the color channels into a single gray-scale channel.\n\n\ndtype\n: returned data type\n\n\n\n\n\n\nimsave\n\n\nSave an image \nndarray\n into file, using PILLOW as backend \n\n\nimsave(f, I, **params)\n\n\n\n\n\n\nf\n: str or file object. The file name or file object to be written into.\n\n\nI\n: Image \nndarray\n. Note for \njpeg\n format, \nI\n should be of \nuint8\n type.\n\n\nparams\n: other parameters passed directly to PILLOW's \nimage.save()",
            "title": "dandelion.ext.CV"
        },
        {
            "location": "/dandelion_ext_CV/#imread",
            "text": "Read image file and return as numpy  ndarray , using PILLOW as backend. Support for EXIF rotation specification.  imread(f, flatten=False, dtype='float32')   f : str or file object. The file name or file object to be read from.  flatten : bool. If  True , flattens the color channels into a single gray-scale channel.  dtype : returned data type",
            "title": "imread"
        },
        {
            "location": "/dandelion_ext_CV/#imsave",
            "text": "Save an image  ndarray  into file, using PILLOW as backend   imsave(f, I, **params)   f : str or file object. The file name or file object to be written into.  I : Image  ndarray . Note for  jpeg  format,  I  should be of  uint8  type.  params : other parameters passed directly to PILLOW's  image.save()",
            "title": "imsave"
        },
        {
            "location": "/history/",
            "text": "History\n\n\nversion 0.16.0 [6-13-2018]\n\n\n\n\nNEW\n: add \next\n module into master branch of Dandelion. All the miscellaneous extensions will be organized in here.\n\n\nNEW\n: add \next.CV\n sub-module, containing image I/O functions and basic image processing functions commonly used in model training.\n\n\n\n\nversion 0.15.2 [5-28-2018]\n\n\n\n\nFIXED\n: \nconvTOP\n should be constructed each time the \nforward()\n function of \nConvTransposed2D\n is called.\n\n\n\n\nversion 0.15.1 [5-25-2018]\n\n\n\n\nNEW\n: add \nmodel\n module into master branch of Dandelion\n\n\nNEW\n: add U-net FCN implementation into \nmodel\n module\n\n\nNEW\n: add \nalign_crop()\n into \nfunctional\n module\n\n\n\n\nversion 0.14.4 [4-17-2018]\n\n\nRename \nupdates.py\n with \nupdate.py\n\n\nversion 0.14.0 [4-10-2018]\n\n\nIn this version the \nModule\n's parameter interfaces are mostly redesigned, so it's \nincompatible\n with previous version.\nNow \nself.params\n and \nself.self_updating_variables\n do not include sub-modules' parameters any more, to get all the parameters to be\ntrained by optimizer, including sub-modules' during training, you'll need to call the new interface function  \n.collect_params()\n. \nTo collect self-defined updates for training, still call \n.collect_self_updates()\n.\n\n\n\n\nMODIFIED\n: \n.get_weights()\n and \n.set_weights()\n traverse the parameters in the same order of sub-modules, so they're \nincompatible\n with previous version.\n\n\nMODIFIED\n: Rewind all \ntrainable\n flags, you're now expected to use the \ninclude\n and \nexclude\n arguments in \n.collect_params()\n and \n\n.collect_self_updates()\n to enable/disable training for certain module's parameters.\n\n\nMODIFIED\n: to define self-update expression for \nself_updating_variable\n, use \n.update\n attribute instead of previous \n.default_update\n\n\nNEW\n: add auto-naming feature to root class \nModule\n: if a sub-module is unnamed yet, it'll be auto-named by its instance name, \nfrom now on you don't need to name a sub-module manually any more.\n\n\nNEW\n: add \n.set_weights_by_name()\n to \nModule\n class, you can use this function to set module weights saved by previous version of Dandelion",
            "title": "History"
        },
        {
            "location": "/history/#history",
            "text": "",
            "title": "History"
        },
        {
            "location": "/history/#version-0160-6-13-2018",
            "text": "NEW : add  ext  module into master branch of Dandelion. All the miscellaneous extensions will be organized in here.  NEW : add  ext.CV  sub-module, containing image I/O functions and basic image processing functions commonly used in model training.",
            "title": "version 0.16.0 [6-13-2018]"
        },
        {
            "location": "/history/#version-0152-5-28-2018",
            "text": "FIXED :  convTOP  should be constructed each time the  forward()  function of  ConvTransposed2D  is called.",
            "title": "version 0.15.2 [5-28-2018]"
        },
        {
            "location": "/history/#version-0151-5-25-2018",
            "text": "NEW : add  model  module into master branch of Dandelion  NEW : add U-net FCN implementation into  model  module  NEW : add  align_crop()  into  functional  module",
            "title": "version 0.15.1 [5-25-2018]"
        },
        {
            "location": "/history/#version-0144-4-17-2018",
            "text": "Rename  updates.py  with  update.py",
            "title": "version 0.14.4 [4-17-2018]"
        },
        {
            "location": "/history/#version-0140-4-10-2018",
            "text": "In this version the  Module 's parameter interfaces are mostly redesigned, so it's  incompatible  with previous version.\nNow  self.params  and  self.self_updating_variables  do not include sub-modules' parameters any more, to get all the parameters to be\ntrained by optimizer, including sub-modules' during training, you'll need to call the new interface function   .collect_params() . \nTo collect self-defined updates for training, still call  .collect_self_updates() .   MODIFIED :  .get_weights()  and  .set_weights()  traverse the parameters in the same order of sub-modules, so they're  incompatible  with previous version.  MODIFIED : Rewind all  trainable  flags, you're now expected to use the  include  and  exclude  arguments in  .collect_params()  and  .collect_self_updates()  to enable/disable training for certain module's parameters.  MODIFIED : to define self-update expression for  self_updating_variable , use  .update  attribute instead of previous  .default_update  NEW : add auto-naming feature to root class  Module : if a sub-module is unnamed yet, it'll be auto-named by its instance name, \nfrom now on you don't need to name a sub-module manually any more.  NEW : add  .set_weights_by_name()  to  Module  class, you can use this function to set module weights saved by previous version of Dandelion",
            "title": "version 0.14.0 [4-10-2018]"
        }
    ]
}